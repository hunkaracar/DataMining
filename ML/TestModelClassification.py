import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold, learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
# LogisticRegression ve MLPClassifier gibi diÄŸer modelleri isterseniz aktif edebilirsiniz.
# from sklearn.linear_model import LogisticRegression
# from sklearn.neural_network import MLPClassifier

# 1. ğŸ“ Veri setinin yolu
file_path = 'Global_Cybersecurity_Threats_2015-2024.csv'

# 2. ğŸ“Š Veri setini oku
try:
    df = pd.read_csv(file_path, encoding='utf-8') # FarklÄ± encoding'leri deneyebilirsiniz: 'latin1', 'iso-8859-1'
except FileNotFoundError:
    print(f"Hata: '{file_path}' dosyasÄ± bulunamadÄ±.")
    exit()
except Exception as e:
    print(f"CSV okunurken hata oluÅŸtu: {e}")
    exit()

# 3. ğŸ¯ Hedef deÄŸiÅŸken ve Ã–zelliklerin Belirlenmesi
target_column = 'Target Industry' # Tahmin edilecek sÃ¼tun
# target_column = 'Attack Type'
# target_column = 'Security Vulnerability Type'

if target_column not in df.columns:
    print(f"Hata: Hedef sÃ¼tun '{target_column}' veri setinde bulunamadÄ±.")
    print(f"Mevcut sÃ¼tunlar: {df.columns.tolist()}")
    exit()

y = df[target_column]
print(f"Hedef DeÄŸiÅŸken: {target_column}")
print(f"SÄ±nÄ±f SayÄ±sÄ±: {y.nunique()}")
print(f"SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:\n{y.value_counts(normalize=True)}\n")


# Ã–zellik matrisi (X) iÃ§in kullanÄ±lmayacak sÃ¼tunlar
# 'Financial Loss (in Million $)' sÃ¼tunu genellikle bir sonuÃ§tur ve sÄ±zÄ±ntÄ±ya neden olabilir.
# EÄŸer bÃ¶yle bir sÃ¼tun yoksa veya analizde kullanÄ±lacaksa, bu listeyi gÃ¼ncelleyin.
columns_to_drop_for_X = [target_column]
# 'Financial Loss (in Million $)' sÃ¼tununun varlÄ±ÄŸÄ±nÄ± kontrol edelim
financial_loss_col = 'Financial Loss (in Million $)' # CSV'deki tam adÄ± bu olmayabilir, kontrol edin
if financial_loss_col in df.columns:
    columns_to_drop_for_X.append(financial_loss_col)
else:
    print(f"UyarÄ±: '{financial_loss_col}' sÃ¼tunu veri setinde bulunamadÄ± ve X'ten Ã§Ä±karÄ±lmayacak.")


X_raw = df.drop(columns=columns_to_drop_for_X, errors='ignore')

# 4. ğŸ”¢ Kategorik verileri sayÄ±sala Ã§evir (One-Hot Encoding)
# Sadece object veya category tipindeki sÃ¼tunlarÄ± one-hot encode et
categorical_cols = X_raw.select_dtypes(include=['object', 'category']).columns
X_encoded = pd.get_dummies(X_raw, columns=categorical_cols, dummy_na=False) # dummy_na=False eksik deÄŸerler iÃ§in ayrÄ± sÃ¼tun oluÅŸturmaz
print(f"Ã–zellik matrisi X'in ÅŸekli (kodlama sonrasÄ±): {X_encoded.shape}")

# 5. åˆ’åˆ† Veriyi EÄŸitim ve Test Setlerine AyÄ±r
# Stratify=y, eÄŸitim ve test setlerindeki sÄ±nÄ±f oranlarÄ±nÄ± korur
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.20, random_state=42, stratify=y
)
print(f"EÄŸitim seti boyutu: {X_train_raw.shape}, Test seti boyutu: {X_test_raw.shape}")

# 6. ğŸ”§ Veriyi Ã¶lÃ§eklendir (standartlaÅŸtÄ±r)
# Scaler'Ä± SADECE eÄŸitim verisi Ã¼zerinde fit et, sonra hem eÄŸitim hem de test verisini transform et
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_raw)
X_test_scaled = scaler.transform(X_test_raw)


# 7. ğŸ” Stratified K-Fold Ã§apraz doÄŸrulama (EÄŸitim verisi Ã¼zerinde kullanÄ±lacak)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 8. ğŸ¤– KullanÄ±lacak modeller
models = {
    "KNN (k=5)": KNeighborsClassifier(n_neighbors=5),
    "SVC (linear)": SVC(kernel='linear', probability=True, random_state=42), # random_state eklendi
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
}

# 9. ğŸ“ˆ Ã‡apraz DoÄŸrulama SonuÃ§larÄ±nÄ± kaydetmek iÃ§in liste
cv_results = []

# 10. ğŸ” Her model iÃ§in Ã‡apraz DoÄŸrulama ile DeÄŸerlendirme (EÄŸitim Verisi Ãœzerinde)
print("\n--- Ã‡apraz DoÄŸrulama SonuÃ§larÄ± (EÄŸitim Verisi Ãœzerinde) ---")
for name, model in models.items():
    print(f"\nModel: {name}")

    # K-Fold doÄŸrulama tahminleri (eÄŸitim verisi Ã¼zerinde)
    y_pred_cv = cross_val_predict(model, X_train_scaled, y_train, cv=kfold)

    # DeÄŸerlendirme metrikleri
    acc = accuracy_score(y_train, y_pred_cv)
    prec = precision_score(y_train, y_pred_cv, average='weighted', zero_division=0)
    rec = recall_score(y_train, y_pred_cv, average='weighted', zero_division=0)
    f1 = f1_score(y_train, y_pred_cv, average='weighted', zero_division=0)

    print(f"CV Accuracy : {acc:.4f}")
    print(f"CV Precision: {prec:.4f}")
    print(f"CV Recall   : {rec:.4f}")
    print(f"CV F1-score : {f1:.4f}")
    print("\nCV Classification Report:\n", classification_report(y_train, y_pred_cv, zero_division=0))

    cm = confusion_matrix(y_train, y_pred_cv)
    plt.figure(figsize=(10, 7)) # SÄ±nÄ±f sayÄ±sÄ± fazlaysa boyutu artÄ±rÄ±n
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_ if hasattr(model, 'classes_') else 'auto', yticklabels=model.classes_ if hasattr(model, 'classes_') else 'auto')
    plt.title(f'CV Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.show()

    cv_results.append({
        "Model": name,
        "CV Accuracy": acc,
        "CV Precision": prec,
        "CV Recall": rec,
        "CV F1-Score": f1
    })

cv_results_df = pd.DataFrame(cv_results)
print("\n--- Ã‡apraz DoÄŸrulama SonuÃ§larÄ± Tablosu ---")
print(cv_results_df)

# Ã–ÄŸrenme EÄŸrisi Fonksiyonu
def plot_learning_curve_custom(estimator, title, X, y, cv, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5)):
    plt.figure(figsize=(8, 5))
    plt.title(title)
    plt.xlabel("EÄŸitim Ã–rnek SayÄ±sÄ±")
    plt.ylabel(scoring.capitalize())

    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=train_sizes, shuffle=True, random_state=42
    )

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std  = np.std(train_scores, axis=1)
    test_scores_mean  = np.mean(test_scores, axis=1)
    test_scores_std   = np.std(test_scores, axis=1)

    plt.grid(True)
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="EÄŸitim skoru")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Ã‡apraz DoÄŸrulama skoru")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.show()

print("\n--- Ã–ÄŸrenme EÄŸrileri (EÄŸitim Verisi Ãœzerinde) ---")
# Ã–ÄŸrenme eÄŸrileri eÄŸitim verisi ve kfold kullanÄ±larak Ã§izilir
for name, model_instance in models.items():
    plot_learning_curve_custom(model_instance,
                               f"Learning Curve - {name}",
                               X_train_scaled, y_train, cv=kfold, scoring='f1_weighted')


# 11. ğŸ§ª Test Seti Ãœzerinde Model PerformansÄ±nÄ± DeÄŸerlendirme Fonksiyonu
def evaluate_model_on_test_set(model_name, model_instance, X_train, y_train, X_test, y_test):
    """
    Verilen modeli eÄŸitim seti Ã¼zerinde eÄŸitir ve test seti Ã¼zerinde deÄŸerlendirir.
    """
    print(f"\n--- Test Ediliyor: {model_name} ---")

    # Modeli tÃ¼m eÄŸitim verisi Ã¼zerinde eÄŸit
    model_instance.fit(X_train, y_train)

    # Test seti Ã¼zerinde tahmin yap
    y_pred_test = model_instance.predict(X_test)

    # Test seti metrikleri
    test_acc = accuracy_score(y_test, y_pred_test)
    test_prec = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)
    test_rec = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)
    test_f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)

    print(f"Test Accuracy : {test_acc:.4f}")
    print(f"Test Precision: {test_prec:.4f}")
    print(f"Test Recall   : {test_rec:.4f}")
    print(f"Test F1-score : {test_f1:.4f}")
    print("\nTest Seti SÄ±nÄ±flandÄ±rma Raporu:\n", classification_report(y_test, y_pred_test, zero_division=0, target_names=np.unique(y_test).astype(str))) # unique etiketleri ekle

    # Test seti iÃ§in confusion matrix
    cm_test = confusion_matrix(y_test, y_pred_test, labels=model_instance.classes_ if hasattr(model_instance, 'classes_') else np.unique(y_test))
    plt.figure(figsize=(10, 7)) # SÄ±nÄ±f sayÄ±sÄ± fazlaysa boyutu artÄ±rÄ±n
    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Oranges', xticklabels=model_instance.classes_ if hasattr(model_instance, 'classes_') else np.unique(y_test).astype(str), yticklabels=model_instance.classes_ if hasattr(model_instance, 'classes_') else np.unique(y_test).astype(str))
    plt.title(f'Test Seti Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.show()

    return {
        "Model": model_name,
        "Test Accuracy": test_acc,
        "Test Precision": test_prec,
        "Test Recall": test_rec,
        "Test F1-Score": test_f1
    }

# 12. ğŸ“Š Test Seti SonuÃ§larÄ±
test_results = []
print("\n--- Test Seti DeÄŸerlendirme SonuÃ§larÄ± ---")
for name, model in models.items():
    # Her model iÃ§in yeni bir Ã¶rnek oluÅŸturmak daha gÃ¼venli olabilir,
    # ancak cross_val_predict modelin kendisini kalÄ±cÄ± olarak deÄŸiÅŸtirmez.
    # EÄŸer emin olmak isterseniz, model tanÄ±mlarÄ±nÄ± burada yeniden yapabilirsiniz.
    # Ã¶rn: test_model = KNeighborsClassifier(n_neighbors=5)
    test_model_instance = model # Mevcut model Ã¶rneÄŸini kullanÄ±yoruz
    
    # SVC iÃ§in classes_ Ã¶zniteliÄŸini ayarlamak gerekebilir eÄŸer y_test'te olmayan sÄ±nÄ±flar varsa
    # Bu genellikle bir sorun olmamalÄ± eÄŸer stratify doÄŸru Ã§alÄ±ÅŸtÄ±ysa ve veri yeterliyse
    # Ancak Ã§ok nadir durumlarda veya Ã§ok kÃ¼Ã§Ã¼k veri setlerinde sorun olabilir.
    # SVC gibi bazÄ± modeller iÃ§in .fit() sonrasÄ± .classes_ oluÅŸur.
    
    result = evaluate_model_on_test_set(name, test_model_instance, X_train_scaled, y_train, X_test_scaled, y_test)
    test_results.append(result)

test_results_df = pd.DataFrame(test_results)
print("\n--- Test Seti SonuÃ§larÄ± Tablosu ---")
print(test_results_df)

# 13. ğŸ“Š Test Seti DoÄŸruluk GrafiÄŸi
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Test Accuracy', data=test_results_df, palette='viridis')
plt.title('Modellerin Test Seti DoÄŸruluk KarÅŸÄ±laÅŸtÄ±rmasÄ±')
plt.xlabel('Model')
plt.ylabel('Test Accuracy')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1.05) # Y eksenini 0 ile 1 arasÄ±nda ayarla
plt.tight_layout()
plt.show()

# 14. â­ En Ä°yi Model (Test Seti DoÄŸruluÄŸuna GÃ¶re)
if not test_results_df.empty:
    best_model_test = test_results_df.loc[test_results_df['Test Accuracy'].idxmax()]
    print("\n--- Test Seti Ãœzerindeki En Ä°yi Model ---")
    print(best_model_test)
else:
    print("\nTest sonuÃ§larÄ± boÅŸ, en iyi model belirlenemedi.")

print("\nAnaliz tamamlandÄ±.")